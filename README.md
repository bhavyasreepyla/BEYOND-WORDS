ASL (American Sign Language) Recognition System

Work in Progress

This project focuses on developing a system for recognizing American Sign Language (ASL) gestures using deep learning techniques. The system employs a Convolutional Neural Network (CNN) for gesture recognition from live video streams captured by a camera. Additionally, it incorporates Natural Language Processing (NLP) to convert recognized gestures into speech, providing real-time audio feedback.

Features

- Gesture Recognitio: Utilizes a trained CNN model to recognize hand gestures corresponding to ASL alphabet letters.
- Text-to-Speech Conversion: Employs NLP techniques to convert recognized gestures into spoken words, aiding communication for individuals with hearing impairments.
- User Interface: Provides an intuitive graphical interface for interacting with the system and visualizing gesture recognition results(using streamlit).

Setup
Prerequisites
- Python 3.x
- TensorFlow
- OpenCV
- NumPy
- Matplotlib
- SpeechRecognition (for text-to-speech functionality)

Usage

1. Launch the application.
2. Position your hand within the camera frame.
3. Perform ASL gestures corresponding to alphabet letters.
4. The system will recognize and convert the gestures into spoken words in real-time.

Future Enhancements

- Model Optimization: Fine-tune the CNN model for improved accuracy and efficiency in gesture recognition.
- Expanded Vocabulary: Include additional ASL gestures and words to enhance communication capabilities.
- User Feedback: Implement audio or visual feedback mechanisms for a better user experience.

Contributing

Contributions are welcome! Feel free to fork this repository and submit pull requests with your enhancements or bug fixes.

